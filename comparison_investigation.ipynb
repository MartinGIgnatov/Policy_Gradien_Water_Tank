{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-05T10:26:05.334517761Z",
     "start_time": "2023-09-05T10:26:01.879850081Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from environment import WaterTank\n",
    "from environment_JAX import WaterTank_Jax\n",
    "from model import MLP, log_weights_and_derivatives\n",
    "from plotting import plot_history\n",
    "from model_JAX import MLP_Jax, log_weights_and_derivatives_JAX\n",
    "from environment_JAX import WaterTank_Jax\n",
    "import optax\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from flax.training import orbax_utils\n",
    "import orbax\n",
    "import wandb\n",
    "from plotting import plot_history\n",
    "import pandas as pd\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NN"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f1c8d26f0b874f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Findings when having only one hidden layer with 32 neurons the speeds are: 36μs, 5.5ms, 55μs;\n",
    "with 100k neurons: 36μs, 5ms, 114μs;\n",
    "JAX NN is ~10% slower with the for loop against hard coding the layers. ~32μs no FOR; 36μs with FOR;\n",
    "Pytorch is 20% with ModuleList than hard coding the layers. ~55μs hard-coded; ~68μs with ModuleList;"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "336be660191a2c38"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.5 µs ± 1.57 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "5.6 ms ± 30.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "68.7 µs ± 3.4 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from params import influx_params, env_params, model_params, run_params, start_params, optimizer_params\n",
    "\n",
    "# JAX\n",
    "level = jnp.array([[start_params[\"level\"]]])\n",
    "time = jnp.array([[start_params[\"time\"]]])\n",
    "state_JAX = jnp.concatenate((level, time), axis = 1)\n",
    "model_JAX = MLP_Jax(model_params[\"layer_sizes\"][1:])\n",
    "weight_params = model_JAX.init(random.PRNGKey(42), state_JAX)\n",
    "\n",
    "# Pytorch\n",
    "water_tank = WaterTank(start_params, env_params, influx_params)\n",
    "state = water_tank.get_state()\n",
    "model = MLP(model_params)\n",
    "\n",
    "# test\n",
    "JAX_apply = jax.jit(model_JAX.apply)\n",
    "JAX_apply(weight_params, state_JAX)\n",
    "%timeit JAX_apply(weight_params, state_JAX)\n",
    "%timeit model_JAX.apply(weight_params, state_JAX)\n",
    "%timeit model(state)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-03T09:50:30.984566748Z",
     "start_time": "2023-09-03T09:50:17.914204728Z"
    }
   },
   "id": "e82879b9e2df8fc2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameter Stats"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a8ee62f7d84ae9a"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "                0\ncount  129.000000\nmean     0.042463\nstd      0.343735\nmin     -0.655224\n25%     -0.143862\n50%      0.015522\n75%      0.294862\nmax      0.707056",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>129.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.042463</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.343735</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-0.655224</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.143862</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.015522</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.294862</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>0.707056</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                0\ncount  129.000000\nmean     0.010316\nstd      0.510590\nmin     -1.601605\n25%     -0.101484\n50%      0.000000\n75%      0.123139\nmax      1.593314",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>129.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.010316</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.510590</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>-1.601605</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>-0.101484</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.123139</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.593314</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from params import influx_params, env_params, model_params, run_params, start_params, optimizer_params\n",
    "\n",
    "# JAX\n",
    "level = jnp.array([[start_params[\"level\"]]])\n",
    "time = jnp.array([[start_params[\"time\"]]])\n",
    "state_JAX = jnp.concatenate((level, time), axis = 1)\n",
    "model_JAX = MLP_Jax(model_params[\"layer_sizes\"][1:])\n",
    "weight_params = model_JAX.init(random.PRNGKey(42), state_JAX)\n",
    "\n",
    "# Pytorch\n",
    "water_tank = WaterTank(start_params, env_params, influx_params)\n",
    "state = water_tank.get_state()\n",
    "model = MLP(model_params)\n",
    "\n",
    "pytorch_par = []\n",
    "for param in model.parameters():\n",
    "    pytorch_par = pytorch_par + param.view(-1).tolist()\n",
    "df_pytorch = pd.DataFrame(pytorch_par)\n",
    "display(df_pytorch.describe())\n",
    "\n",
    "weights_flatten = jax.tree_util.tree_flatten(weight_params)[0]\n",
    "weights_flatten = jax.tree_map(jnp.ravel, weights_flatten)\n",
    "weights_flatten = jnp.concatenate(weights_flatten)\n",
    "df_JAX = pd.DataFrame(weights_flatten.tolist())\n",
    "display(df_JAX.describe())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T13:12:25.309141563Z",
     "start_time": "2023-09-04T13:12:25.251456299Z"
    }
   },
   "id": "cb800361a9380b9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Size Comparison"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33d63c7465a57b00"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.weight [32, 2] layers.0.bias [32] layers.2.weight [1, 32] layers.2.bias [1] \n",
      "\n",
      "{'params': {'layers_0': {'bias': (32,), 'kernel': (2, 32)}, 'layers_1': {'bias': (1,), 'kernel': (32, 1)}}}\n"
     ]
    }
   ],
   "source": [
    "from params import influx_params, env_params, model_params, run_params, start_params, optimizer_params\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# JAX\n",
    "level = jnp.array([[start_params[\"level\"]]])\n",
    "time = jnp.array([[start_params[\"time\"]]])\n",
    "state_JAX = jnp.concatenate((level, time), axis = 1)\n",
    "model_JAX = MLP_Jax(model_params[\"layer_sizes\"][1:])\n",
    "weight_params = model_JAX.init(random.PRNGKey(42), state_JAX)\n",
    "\n",
    "# Pytorch\n",
    "water_tank = WaterTank(start_params, env_params, influx_params)\n",
    "state = water_tank.get_state()\n",
    "model = MLP(model_params)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, list(param.shape), end = \" \")\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(jax.tree_map(lambda x: x.shape, weight_params))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T16:57:44.159229884Z",
     "start_time": "2023-09-04T16:57:44.122299622Z"
    }
   },
   "id": "b0af636626b21bf1"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.2911819e-01  1.2873628e+00  4.1615748e-01 -3.2046041e-01\n",
      "  -1.0824715e+00 -3.4905264e-01 -4.0321961e-02  1.0576610e-03\n",
      "  -8.2933229e-01  3.5555342e-01 -5.5605835e-01 -8.7483805e-01\n",
      "  -1.6016047e+00  3.8891137e-01  1.0036556e-01 -3.3083498e-01\n",
      "   8.1692898e-01  2.4937668e-01  2.0649074e-01  2.7163011e-01\n",
      "   3.1569880e-01  1.0814291e+00 -2.6750933e-02 -1.3788413e+00\n",
      "  -1.1066534e-01 -8.4871006e-01 -4.2546454e-01  7.7156889e-01\n",
      "   1.9841006e-02  1.5933142e+00  6.0597646e-01  4.5173422e-01]\n",
      " [-7.2890383e-01  1.2313894e-01  1.5339019e+00 -1.4125415e-02\n",
      "  -6.6341805e-01  8.5607606e-01  6.0746133e-01  1.1522740e-01\n",
      "  -9.4534695e-01 -1.0287511e+00 -2.6974782e-01 -4.9252715e-02\n",
      "  -3.5951115e-02  5.8555305e-01  7.5108573e-02  1.2834657e+00\n",
      "   1.2580771e+00  7.0255592e-02  1.0237257e+00  5.2203673e-01\n",
      "  -2.1699713e-01  5.0387986e-02 -9.9643892e-01  2.0291030e-01\n",
      "  -4.1196761e-01 -1.0742004e+00  3.7874031e-01 -6.6206402e-01\n",
      "   6.6892937e-02  5.0207633e-01 -6.2699866e-01 -8.7600297e-01]]\n",
      "[[0.5406103730201721, -0.16565565764904022, -0.15492962300777435, -0.34425848722457886, 0.6233449578285217, 0.6146144866943359, 0.5224167704582214, 0.34095847606658936, 0.545098602771759, -0.33010566234588623, -0.3257899880409241, -0.2871972918510437, -0.5581690073013306, -0.19966909289360046, 0.06673894077539444, 0.6385945677757263, 0.5459057688713074, -0.22960077226161957, 0.11020313203334808, 0.07730069011449814, 0.18998804688453674, 0.2975912094116211, 0.4087498188018799, 0.40818774700164795, 0.359093576669693, -0.6999709606170654, -0.5423670411109924, 0.20366890728473663, 0.2236304134130478, 0.5533875823020935, 0.04452207684516907, 0.21803778409957886], [0.5869042277336121, 0.6495562791824341, 0.14268755912780762, 0.4152715504169464, -0.5187534093856812, 0.13234160840511322, 0.09576387703418732, -0.09983711689710617, 0.10451667755842209, 0.18024031817913055, -0.08292442560195923, 0.4690741002559662, -0.32598352432250977, -0.4251638352870941, -0.6983945965766907, -0.6006647348403931, 0.11767610907554626, 0.43695661425590515, 0.5713163614273071, -0.22300250828266144, -0.1917528212070465, 0.631320595741272, -0.30912765860557556, 0.12651994824409485, -0.43098515272140503, -0.2731981575489044, 0.5802085995674133, 0.2928932011127472, -0.012300810776650906, -0.5024089813232422, -0.48262879252433777, -0.2435150444507599]]\n",
      "[[ 0.5406104  -0.16565566 -0.15492962 -0.3442585   0.62334496  0.6146145\n",
      "   0.5224168   0.34095848  0.5450986  -0.33010566 -0.32579    -0.2871973\n",
      "  -0.558169   -0.1996691   0.06673894  0.63859457  0.54590577 -0.22960077\n",
      "   0.11020313  0.07730069  0.18998805  0.2975912   0.40874982  0.40818775\n",
      "   0.35909358 -0.69997096 -0.54236704  0.2036689   0.22363041  0.5533876\n",
      "   0.04452208  0.21803778]\n",
      " [ 0.5869042   0.6495563   0.14268756  0.41527155 -0.5187534   0.13234161\n",
      "   0.09576388 -0.09983712  0.10451668  0.18024032 -0.08292443  0.4690741\n",
      "  -0.32598352 -0.42516384 -0.6983946  -0.60066473  0.11767611  0.4369566\n",
      "   0.57131636 -0.22300251 -0.19175282  0.6313206  -0.30912766  0.12651995\n",
      "  -0.43098515 -0.27319816  0.5802086   0.2928932  -0.01230081 -0.502409\n",
      "  -0.4826288  -0.24351504]]\n"
     ]
    }
   ],
   "source": [
    "print(weight_params[\"params\"][\"layers_0\"][\"kernel\"])\n",
    "print(list(model.parameters())[0].T.tolist())\n",
    "print(weight_params[\"params\"][\"layers_0\"][\"kernel\"].at[:].set(list(model.parameters())[0].T.tolist()))\n",
    "#print(weight_params[\"params\"][\"layers_0\"][\"kernel\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T16:57:44.610129476Z",
     "start_time": "2023-09-04T16:57:44.577978887Z"
    }
   },
   "id": "4b1b87f3b4a57a62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Same Initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55ea621ced40802"
  },
  {
   "cell_type": "markdown",
   "source": [
    "When both learning phases of the algorithms are ran with the same parameters they produce different results. The Pytorch one seems to converge better than the JAX, even though they use the same parameters and there is no stochasticity. If the learned parameters from the pytorch are copied to JAX they produce the same episodes. So, it is not take_step the issue. Results for during learning phase can be seen in the \"charts_comparisson_same_parameter_initialization\" image and the \"grad_charts...\" image. It looks like small differences in gradients to cause the change as initially both have very similar workflows but diverge later. Maybe it is the precision of the numbers. They are both float32.\n",
    "\n",
    "Adding the following code to the main_JAX will allow it to have the same initial parameters as the pytorch one."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a62e8bbe896407d3"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jnp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m torch\u001B[38;5;241m.\u001B[39mmanual_seed(seed)\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# starting conditions\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m level \u001B[38;5;241m=\u001B[39m \u001B[43mjnp\u001B[49m\u001B[38;5;241m.\u001B[39marray([[start_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlevel\u001B[39m\u001B[38;5;124m\"\u001B[39m]]])\n\u001B[1;32m      9\u001B[0m curr_time \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39marray([[start_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtime\u001B[39m\u001B[38;5;124m\"\u001B[39m]]])\n\u001B[1;32m     10\u001B[0m state \u001B[38;5;241m=\u001B[39m jnp\u001B[38;5;241m.\u001B[39mconcatenate((level, curr_time), axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'jnp' is not defined"
     ]
    }
   ],
   "source": [
    "from params import influx_params, env_params, model_params, run_params, start_params, optimizer_params\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# starting conditions\n",
    "level = jnp.array([[start_params[\"level\"]]])\n",
    "curr_time = jnp.array([[start_params[\"time\"]]])\n",
    "state = jnp.concatenate((level, curr_time), axis = 1)\n",
    "\n",
    "# ML\n",
    "model = MLP_Jax(model_params[\"layer_sizes\"][1:]) # have to remove the first element\n",
    "weight_params = model.init(random.PRNGKey(42), state)\n",
    "model = MLP(model_params)\n",
    "\n",
    "weight_params[\"params\"][\"layers_0\"][\"kernel\"] = weight_params[\"params\"][\"layers_0\"][\"kernel\"].at[:].set(list(model.parameters())[0].T.tolist())\n",
    "\n",
    "weight_params[\"params\"][\"layers_0\"][\"bias\"] = weight_params[\"params\"][\"layers_0\"][\"bias\"].at[:].set(list(model.parameters())[1].tolist())\n",
    "\n",
    "weight_params[\"params\"][\"layers_1\"][\"kernel\"] = weight_params[\"params\"][\"layers_1\"][\"kernel\"].at[:].set(list(model.parameters())[2].T.tolist())\n",
    "\n",
    "weight_params[\"params\"][\"layers_1\"][\"bias\"] = weight_params[\"params\"][\"layers_1\"][\"bias\"].at[:].set(list(model.parameters())[3].tolist())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T10:05:05.641311008Z",
     "start_time": "2023-09-05T10:05:03.864521222Z"
    }
   },
   "id": "8c5bb4fd92c0a416"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parameter Precision, decimal point"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "960aaebe9bd743c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from params import influx_params, env_params, model_params, run_params, start_params, optimizer_params\n",
    "import torch\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# starting conditions\n",
    "level = jnp.array([[start_params[\"level\"]]])\n",
    "curr_time = jnp.array([[start_params[\"time\"]]])\n",
    "state = jnp.concatenate((level, curr_time), axis = 1)\n",
    "\n",
    "# ML\n",
    "model_JAX = MLP_Jax(model_params[\"layer_sizes\"][1:]) # have to remove the first element\n",
    "weight_params = model_JAX.init(random.PRNGKey(42), state)\n",
    "model = MLP(model_params)\n",
    "\n",
    "print(weight_params[\"params\"][\"layers_0\"][\"kernel\"].dtype)\n",
    "print(list(model.parameters())[0].dtype)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ace881270c311ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NN Size to Loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be8c4fe5934b2452"
  },
  {
   "cell_type": "markdown",
   "source": [
    "1 and 2 layers have similar performances. 3 layers has a 10% better performance and bigger learning time. Two three times more time before obvious convergence appears."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f880f96cb64a80cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Leaky Relu vs Relu"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d1685632f7573b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Leaky has better performance for one layer ~10% better convergence but slightly slower.\n",
    "Same result for two layers but had to double the epochs to see the improvement.\n",
    "Three and four layers never converge for the leaky, even the gradients were 0 and stayed as such.\n",
    "Disclaimer, normal 4 layer case also has a strange behaviour where it has no gradient but suddenly it does have a working gradient."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8a6a783c3270db5a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Learning Rate Scheduler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a52a82be3f85a86"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Adding a cosine scheduler has produced the best results so far for hyper-tuning. An image can be seen called \"learning_schedule_results\". The code section for the tuning is:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aa8313740e14862"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr_scheduler = optax.cosine_decay_schedule(0.001, 50000)\n",
    "optimizer = optax.adam(learning_rate= lr_scheduler)\n",
    "opt_state = optimizer.init(weight_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2da20de3540d7823"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Keep in mind that the number of decay steps is dependent on the amount of backprops, which are dependent on both epochs and steps in an episode. So the number of 50k, actually should bring the learning rate to 0 at 20% from the learning, as there are a total of 25k backprops(500 epochs, 500 steps). So the episode I ran could be with only 100 epochs."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "757b893755fab7b0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "myenv",
   "language": "python",
   "display_name": "Water_Tank.3.11.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
